---
title: 可用性设计
date: 2022-05-30 15:54:37
permalink: /pages/e1f3c7/
categories:
  - 微服务
tags:
  - 
author: 
  name: weiluoliang
  link: https://github.com/weiluoliang
---

## 隔离

隔离，本质上是对系统或资源进行分割，从而实现当系统发生故障时能限定传播范围和影响范围，即发生故障后只有出问题的服务不可用，保证其他服务仍然可用。

###  **服务隔离**  

   - 动静分离

     小到 CPU 的 cacheline false sharing、数据库 mysql 表设计中避免 bufferpool 频繁过期，隔离动静表，大到架构设计中的图片、静态资源等缓存加速。本质上都体现的一样的思路，即加速/缓存访问变换频次小的。比如 CDN 场景中，将静态资源和动态 API 分离，也是体现了隔离的思路:

     - **降低应用服务器的负载，访问静态资源都是通过CDN**
     - **对象存储费用低**
     - **海量存储空间，无需考虑存储扩容问题**
     - **静态CDN带宽加速，延迟低** 

![](https://media-1251528481.cos.ap-hongkong.myqcloud.com/blog/%E5%8F%AF%E7%94%A8%E6%80%A7%E8%AE%BE%E8%AE%A1-%E5%8A%A8%E9%9D%99%E9%9A%94%E7%A6%BB01.jpg)



   - 低频数据与高频数据的隔离（B站视频稿件例子）

     archive: 稿件表，存储稿件的名称、作者、分类、tag、状态等信息，表示稿件的基本信息。

        ***在一个投稿流程中，一旦稿件创建改动的频率比较低。***

      archive_stat: 稿件统计表，表示稿件的播放、点赞、收藏、投币数量，比较高频的更新。

        ***随着稿件获取流量，稿件被用户所消费，各类计数信息更新比较频繁。***

     MySQL BufferPool 是用于缓存 DataPage 的，DataPage 可以理解为缓存了表的行，那么如果频繁更新 DataPage 不断会置换，会导致命中率下降的问题，所以我们在表设计中，仍然可以沿用类似的思路，其主表基本更新，在上游 Cache 未命中，透穿到 MySQL，仍然有 BufferPool 的缓存。

     <img src="https://media-1251528481.cos.ap-hongkong.myqcloud.com/blog%2F%E6%9C%8D%E5%8A%A1%E9%9A%94%E7%A6%BB-%E4%BD%8E%E9%A2%91%E4%B8%8E%E9%AB%98%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9A%94%E7%A6%BB01.png" style="zoom: 67%;" />

     

   -  读写分离

### **轻重隔离**  

- 核心隔离

  业务按照 Level 进行资源池划分(L0/L1/L2)

  - 核心/非核心的故障域的差异隔离(机器资源、依赖资源)。

  - 多集群，通过冗余资源来提升吞吐和容灾能力。

    <img src="https://media-1251528481.cos.ap-hongkong.myqcloud.com/核心隔离01.png" style="zoom:35%;" />

- 快慢隔离

  我们可以把服务的吞吐想象为一个池，当突然洪流进来时，池子需要一定时间才能排放完，这时候其他支流在池子里待的时间取决于前面的排放能力，耗时就会增高，对小请求产生影响。

  日志传输体系的架构设计中，整个流都会投放到一个 kafka topic 中(早期设计目的: 更好的顺序IO)，流内会区分不同的 logid，logid 会有不同的 sink 端，它们之前会出现差速，比如 HDFS 抖动吞吐下降，ES 正常水位，全局数据就会整体反压。

  - 按照各种纬度隔离：sink、部门、业务、logid、重要性(S/A/B/C)。

  业务日志也属于某个 logid，日志等级就可以作为隔离通道。

  ![](https://media-1251528481.cos.ap-hongkong.myqcloud.com/快慢隔离01.png)

- 热点隔离

  何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行缓存。比如：

  - 小表广播: 从 remotecache 提升为 localcache，app 定时更新，甚至可以让运营平台支持广播刷新 localcache。atomic.Value
  - 主动预热: 比如直播房间页高在线情况下bypass 监控主动防御。

  

### **物理隔离**  

- 线程隔离

  主要通过线程池进行隔离，也是实现服务隔离的基础。把业务进行分类并交给不同的线程池进行处理，当某个线程池处理一种业务请求发生问题时，不会讲故障扩散和影响到其他线程池，保证服务可用。

-  进程、集群、机房

![](https://media-1251528481.cos.ap-hongkong.myqcloud.com/blog/线程隔离.png)





## 超时控制

> 超时控制是微服务可用性的第一道关，良好的超时策略，可以尽可能让服务不堆积请求，尽快清空高延迟的请求

超时控制，我们的组件能够快速失效(fail fast)，没有什么比挂起的请求和无响应的界面更令人失望。这不仅浪费资源，而且还会让用户体验变得更差。我们的服务是互相调用的，所以在这些延迟叠加前，应该特别注意防止那些超时的操作。

- 网路传递具有不确定性。
- 客户端和服务端不一致的超时策略导致资源浪费。
- “默认值”策略。
- 高延迟服务导致 client 浪费资源等待，使用超时传递: 进程间传递 + 跨进程传递。

<img src="https://media-1251528481.cos.ap-hongkong.myqcloud.com/blog/网络超时.png" style="zoom:35%;" />

实际业务开发中，我们依赖的微服务的超时策略并不清楚，或者随着业务迭代耗时超生了变化，意外的导致依赖者出现了超时。

- 配置中心公共模版，对于未配置的服务使用公共配置。

**超时传递**: 当上游服务已经超时返回 504，但下游服务仍然在执行，会导致浪费资源做无用功。超时传递指的是把当前服务的剩余 Quota 传递到下游服务中，继承超时策略，控制请求级别的全局超时控制。

- 进程内超时控制: 一个请求在每个阶段(网络请求)开始前，就要检查是否还有足够的剩余来处理请求，以及继承他的超时策略

### 超时故障

- SLB 入口 Nginx 没配置超时导致连锁故障。
- 服务依赖的 DB 连接池漏配超时，导致请求阻塞，最终服务集体 OOM。
- 下游服务发版耗时增加，而上游服务配置超时过短，导致上游请求失败。

## 限流

### 令牌桶算法

是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。令牌桶算法的描述如下：

- 假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌。
- 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
- 当一个 n 个字节大小的数据包到达，将从桶中删除n 个令牌，接着数据包被发送到网络上。
- 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

### 漏桶算法

- 一个固定容量的漏桶，按照常量固定速率流出水滴。
- 如果桶是空的，则不需流出水滴。
- 可以以任意速率流入水滴到漏桶。
- 如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。

漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路都是设定一个指标, 当超过该指标后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，但往往设置合理不是一件容易的事情。

- 集群增加机器或者减少机器限流阈值是否要重新设置?
- 设置限流阈值的依据是什么?
- 人力运维成本是否过高?
- 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义?



### 熔断

> 断路器以现实世界的电子元件命名，因为它们的行为是都是相同的。断路器在分布式系统中非常有用，因为重复的故障可能会导致雪球效应，并使整个系统崩溃。

- 服务依赖的资源出现大量错误。
- 某个用户超过资源配额时，后端任务会快速拒绝请求，返回“配额不足”的错误，但是拒绝回复仍然会消耗一定资源。有可能后端忙着不停发送拒绝请求，导致过载。



### 客户端流控

用户总是积极重试，访问一个不可达的服务，客户端需要限制请求频次。如不控制，用户重试导致的大面积故障。



## 降级

**降级的本质： 有损提供服务**

通过降级回复来减少工作量，或者丢弃不重要的请求。而且需要了解哪些流量可以降级，并且有能力区分不同的请求。我们通常提供降低回复的质量来答复减少所需的计算量或者时间。我们自动降级通常需要考虑几个点：

- 确定具体采用哪个指标作为流量评估和优雅降级的决定性指标(如，CPU、延迟、队列长度、线程数量、错误等)。
- 当服务进入降级模式时，需要执行什么动作？
- 流量抛弃或者优雅降级应该在服务的哪一层实现？是否需要在整个服务的每一层都实现，还是可以选择某个高层面的关键节点来实现？

同时我们需要考虑：

- 优雅降级不应该被经常触发 - 通常触发条件现实了容量规划的失误，或者是意外的负载。
- 演练，代码平时不会触发和使用，需要定期针对一小部分的流量进行演练，保证模式的正常。
- 应该足够简单。

## 重试

当请求返回错误(例: 配额不足、超时、内部错误等)，对于 backend 部分节点过载的情况下，倾向于立刻重试，但是需要留意重试带来的流量放大:

- 限制重试次数和基于重试分布的策略

重试需要主意的问题：

- 业务数据需要幂等，多次请求的结果时一样的
  - 全局唯一 ID: 根据业务生成一个全局唯一 ID，在调用接口时会传入该 ID，接口提供方会从相应的存储系统比如 redis 中去检索这个全局 ID 是否存在
  - 去重表: 这种方法适用于在业务中有唯一标识的插入场景。
  - 多版本并发控制: 适合对更新请求作幂等性控制,比如要更新商品的名字，这是就可以在更新的接口中增加一个版本号来做幂等性控制。

## 负载均衡

在理想情况下，某个服务的负载会完全均匀地分发给所有的后端任务。在任何时刻，最忙和最不忙的节点永远消耗同样数量的CPU。

目标：

- 均衡的流量分发。
- 可靠的识别异常节点。
- 减少错误，提高可用性

我们发现在 backend 之间的 load 差异比较大

- 每个请求的处理成本不同
- 物理机环境的差异:
  - 服务器很难强同质性
  - 存在共享资源争用（内存缓存、带宽、IO等）
- 性能因素:
  - FullGC
  - JVM JIT

## 最佳实践

### 变更管理:

70％的问题是由变更引起的，恢复可用代码并不总是坏事。

### 避免过载:

过载保护、流量调度等。

###  依赖管理:

任何依赖都可能故障，做 chaos monkey testing，注入故障测试。

### 优雅降级:

有损服务，避免核心链路依赖故障。

### 重试退避:

### 超时控制:

进程内 + 服务间 超时控制。

### 极限压测 + 故障演练

### 扩容 + 重启 + 消除有害流量

## References

http://www.360doc.com/content/16/1124/21/31263000_609259745.shtml

http://www.infoq.com/cn/articles/basis-frameworkto-implement-micro-service/

http://www.infoq.com/cn/news/2017/04/linkerd-celebrates-one-year

https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c

https://mp.weixin.qq.com/s?__biz=MzAwNjQwNzU2NQ==&mid=402841629&idx=1&sn=f598fec9b370b8a6f2062233b31122e0&mpshare=1&scene=23&srcid=0404qP0fH8zRiIiFzQBiuzuU#rd

https://mp.weixin.qq.com/s?__biz=MzIzMzk2NDQyMw==&mid=2247486641&idx=1&sn=1660fb41b0c5b8d8d6eacdfc1b26b6a6&source=41#wechat_redirect

https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices/

https://www.cs.columbia.edu/~ruigu/papers/socc18-final100.pdf

https://github.com/alibaba/Sentinel/wiki/系统负载保护

https://blog.csdn.net/okiwilldoit/article/details/81738782

http://alex-ii.github.io/notes/2019/02/13/predictive_load_balancing.html

https://blog.csdn.net/m0_38106113/article/details/81542863
